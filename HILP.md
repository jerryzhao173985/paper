<start>
# Understanding Hilbert Foundation Policies (HILPs) for Unsupervised Pre-Training

This paper introduces Hilbert foundation policies (HILPs), a general unsupervised pre-training objective for foundation policies that aims to capture diverse, optimal long-horizon behaviors from unlabeled data to facilitate downstream task learning. The key aspects and contributions of the method are:

- **Hilbert representations**: They first learn a geometric abstraction of the dataset by training a representation function <img src="https://latex.codecogs.com/svg.image?\phi:S\rightarrow&space;Z" title="\phi:S\rightarrow Z" /> that maps states to a Hilbert space <img src="https://latex.codecogs.com/svg.image?Z" title="Z" /> such that distances in <img src="https://latex.codecogs.com/svg.image?Z" title="Z" /> correspond to the temporal distances between states in the original MDP. This distance-preserving mapping abstracts the state space while preserving the long-term global relationships between states.

- **Unsupervised policy training**: After obtaining the Hilbert representation <img src="https://latex.codecogs.com/svg.image?\phi" title="\phi" />, they train a latent-conditioned policy <img src="https://latex.codecogs.com/svg.image?\pi(a|s,z)" title="\pi(a|s,z)" /> using offline RL to span the latent space <img src="https://latex.codecogs.com/svg.image?Z" title="Z" /> with skills that correspond to directional movements. The reward function is defined as the inner product between <img src="https://latex.codecogs.com/svg.image?\phi(s')-\phi(s)" title="\phi(s')-\phi(s)" /> and a randomly sampled unit vector <img src="https://latex.codecogs.com/svg.image?z" title="z" />. By learning to move in every possible direction, the policy learns diverse long-horizon behaviors that optimally span both the latent and state spaces. The resulting multi-task policy <img src="https://latex.codecogs.com/svg.image?\pi(a|s,z)" title="\pi(a|s,z)" /> is called a Hilbert foundation policy (HILP).

- **Zero-shot prompting**: HILPs provide multiple ways to quickly adapt to downstream tasks in a zero-shot manner:
    - Zero-shot RL: Given a reward function at test time, the optimal latent vector <img src="https://latex.codecogs.com/svg.image?z" title="z" /> that maximizes it can be found via linear regression without additional training, enabled by the successor feature structure of the HILP reward function.
    - Zero-shot goal-conditioned RL: Given a target goal state, moving in the latent direction of <img src="https://latex.codecogs.com/svg.image?\phi(g)-\phi(s)" title="\phi(g)-\phi(s)" /> is proved to be optimal for reaching the goal if embedding errors are sufficiently small.
    - Test-time planning: The structured Hilbert representation enables efficiently finding an optimal subgoal between the current state and goal to refine the policy prompts, which can be done iteratively to further improve performance.

- **Strong empirical results**: On simulated robotic benchmarks, HILPs outperform prior methods specialized for zero-shot RL, goal-conditioned RL and hierarchical RL, demonstrating the effectiveness of capturing state-spanning long-horizon behaviors for unsupervised pre-training.

The key insights are:

1. Capturing the long-term temporal structure of the environment in a Hilbert space enables learning diverse useful behaviors.
2. The inner product structure allows versatile prompting of the learned policy for efficient zero-shot adaptation.
3. Pre-training with a principled objective leads to a general-purpose foundation policy effective for multiple downstream settings.

The Hilbert space plays a central role in the proposed method for unsupervised pre-training of foundation policies. In this paper, the authors use a Hilbert space <img src="https://latex.codecogs.com/svg.image?Z" title="Z" /> as the latent space to embed the state space <img src="https://latex.codecogs.com/svg.image?S" title="S" /> of the MDP. The key property they seek is to learn a representation function <img src="https://latex.codecogs.com/svg.image?\phi:S\rightarrow&space;Z" title="\phi:S\rightarrow Z" /> that preserves the temporal structure of the MDP in the geometry of the Hilbert space. Specifically, they aim to learn <img src="https://latex.codecogs.com/svg.image?\phi" title="\phi" /> such that the Euclidean distance between two states' embeddings in <img src="https://latex